{"version":3,"sources":["speech.babel.js"],"names":["openSpeechRecognition","window","SpeechRecognition","webkitSpeechRecognition","recognition","language","displayMessage","reply","input","$","append","animate","scrollTop","scrollHeight","speech","callback","addEventListener","e","transcript","Array","from","results","map","result","join","text","isFinal","stop","start","speechClick","console","log","sendText","data","message","context","JSON","stringify","ajax","type","url","success","res","resp","output","msg","SpeechSynthesisUtterance","speechSynthesis","speak","uploadPic","triggerUrlField","error","err","sendUrl"],"mappings":"YAAA,SAASA,yBACLC,OAAOC,kBAAoBD,OAAOC,mBAAqBD,OAAOE,uBAC9D,IAAMC,GAAc,GAAIF,kBAIxB,OADAE,GAAYC,SAAW,KAChBD,EAGX,QAASE,gBAAeC,EAAOC,GACxBD,GACCE,EAAE,eAAeC,OAAO,qCAAuCH,EAAQ,eACvEE,EAAE,eAAeE,SACjBC,UAAWH,EAAE,eAAe,GAAGI,cAAe,OAE9CJ,EAAE,eAAeC,OAAO,oCAAsCF,EAAQ,eACtEC,EAAE,eAAeE,SACjBC,UAAWH,EAAE,eAAe,GAAGI,cAAe,MAItD,QAASC,QAAOC,GACZ,GAAIX,GAAcJ,uBAClBI,GAAYY,iBAAiB,SAAU,SAAAC,GACnC,GAAIC,GAAaC,MAAMC,KAAKH,EAAEI,SAC7BC,IAAI,SAAAC,GAAA,MAAUA,GAAO,KACrBD,IAAI,SAAAC,GAAA,MAAUA,GAAOL,aACrBM,KAAK,GAENf,GAAE,WAAWgB,KAAKP,GAClBZ,gBAAe,EAAOY,GAElBD,EAAEI,QAAQ,GAAGK,UACbX,EAASG,GACTd,EAAYuB,UAIpBvB,EAAYwB,QAGhB,QAASC,eACLC,QAAQC,IAAI,WACZtB,EAAE,WAAWgB,KAAK,IAClBX,OAAO,SAASW,GACZO,SAASP,KAMjB,QAASO,UAASP,GACd,GAAIQ,IACIC,QAAST,EACTU,QAASC,KAAKC,UAAUF,SAEhC1B,GAAE6B,MACEC,KAAM,OACNC,IAAK,gBACLP,KAAMA,EACNQ,QAAS,SAASC,GACd,GAAIC,GAAOD,EAAIE,OAAOnB,KAAK,EAC3BU,SAAUO,EAAIP,QACdL,QAAQC,IAAII,SACZL,QAAQC,IAAIY,GACZrC,eAAeqC,GAAM,EAErB,IAAIE,GAAM,GAAIC,0BAAyBH,EACvC1C,QAAO8C,gBAAgBC,MAAMH,GAEJ,QAArBV,QAAQc,YACRd,QAAQc,UAAY,QACpBC,oBAGRC,MAAO,SAASC,GACZtB,QAAQC,IAAIqB,MAKxB,QAASF,mBAGLG,QAAQb,KAGZ,QAASa,SAAQb,GACb/B,EAAE6B,MACEC,KAAM,OACNC,IAAK,qBACLP,MACIE,QAASC,KAAKC,UAAUF,SACxBK,IAAKA,GAETC,QAAS,SAASC,GACd,GAAIC,GAAOD,EAAIE,OAAOnB,KAAK,EAC3BU,SAAUO,EAAIP,QACdL,QAAQC,IAAII,SACZL,QAAQC,IAAIY,EAEZ,IAAIE,GAAM,GAAIC,0BAAyBH,EACvC1C,QAAO8C,gBAAgBC,MAAMH,GAEzBV,QAAQc,YACRd,QAAQc,WAAY,EACpBA,cAGRE,MAAO,SAASC,GACZtB,QAAQC,IAAIqB,MA7DxB,GAAIjB","file":"speech.babel.js","sourcesContent":["function openSpeechRecognition() {\n    window.SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;\n    const recognition = new SpeechRecognition();\n    // recognition.continuous = true;\n    // recognition.interimResults = true;\n    recognition.language = 'de';\n    return recognition;\n}\n\nfunction displayMessage(reply, input) {\n    if(reply) {\n        $('.messageBox').append('<div class=\"is-12 start left\"><h2>' + reply + '</h2></div>');\n        $('.messageBox').animate({\n        scrollTop: $('.messageBox')[0].scrollHeight}, 2000);\n    } else {\n        $('.messageBox').append('<div class=\"is-12 end right\"><h2>' + input + '</h2></div>');\n        $('.messageBox').animate({\n        scrollTop: $('.messageBox')[0].scrollHeight}, 2000);\n    }\n}\n\nfunction speech(callback){\n    let recognition = openSpeechRecognition();\n    recognition.addEventListener('result', e => {\n        let transcript = Array.from(e.results)\n        .map(result => result[0])\n        .map(result => result.transcript)\n        .join('');\n\n        $('#mytext').text(transcript);\n        displayMessage(false, transcript);\n\n        if (e.results[0].isFinal) {\n            callback(transcript);\n            recognition.stop();\n\n        }\n    });\n    recognition.start();\n}\n\nfunction speechClick(){\n    console.log(\"clicked\");\n    $('#mytext').text(\"\");\n    speech(function(text){\n        sendText(text);\n    });\n}\n\nvar context;\n\nfunction sendText(text){\n    var data = {\n            message: text, \n            context: JSON.stringify(context)\n        }\n    $.ajax({\n        type: \"POST\",\n        url: \"/conversation\",\n        data: data,\n        success: function(res){\n            var resp = res.output.text[0];\n            context = res.context;\n            console.log(context);\n            console.log(resp);\n            displayMessage(resp, false);\n\n            var msg = new SpeechSynthesisUtterance(resp);\n            window.speechSynthesis.speak(msg);\n\n            if (context.uploadPic == \"true\") {\n                context.uploadPic = \"false\";\n                triggerUrlField();\n            }\n        },\n        error: function(err){\n            console.log(err);\n        }\n    });\n}\n\nfunction triggerUrlField() {\n    //trigger url field stuff\n\n    sendUrl(url);\n}\n\nfunction sendUrl(url){\n    $.ajax({\n        type: \"POST\",\n        url: \"/visualRecognition\",\n        data: {\n            context: JSON.stringify(context),\n            url: url\n        },\n        success: function(res){\n            var resp = res.output.text[0];\n            context = res.context;\n            console.log(context);\n            console.log(resp);\n\n            var msg = new SpeechSynthesisUtterance(resp);\n            window.speechSynthesis.speak(msg);\n\n            if (context.uploadPic) {\n                context.uploadPic = false;\n                uploadPic();\n            }\n        },\n        error: function(err){\n            console.log(err);\n        }\n    });\n}\n"]}